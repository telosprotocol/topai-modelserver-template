topai:
  name: llama-3-8b-server  # Name of the model service
  model:
    type: llm  # Model type: Large Language Model
    source:
      local:
        path: /home/models/  # Local storage path for the model
  serving:
    import_path: src.main:vllmservice  # Import path for the serving framework (default:Ray Serve)
  inference:
    engine:
      vllm:  # vLLM inference engine configuration
        model: NousResearch/Meta-Llama-3-8B  # Model name/path
        serve_model_name: Meta-Llama-3-8B  # Name of the model when served
        max_model_len: 8192  # Maximum sequence length for the model
