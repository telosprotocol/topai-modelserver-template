# This file was generated using the `serve build` command on Ray v2.34.0.

proxy_location: EveryNode

http_options:
  host: 0.0.0.0
  port: 8000
grpc_options:
  port: 9000
  grpc_servicer_functions: []

logging_config:
  encoding: TEXT
  log_level: INFO
  logs_dir: null
  enable_access_log: true

applications:
  - name: app
    runtime_env:
      env_vars:
        VLLM_TEST_FORCE_FP8_MARLIN: "1"
    route_prefix: /
    import_path: src.main:vllmservice
    deployments:
      - name: VLLMService

        autoscaling_config:
          target_ongoing_requests: 5
          min_replicas: 1
          max_replicas: 2
          max_ongoing_requests: 10

        ray_actor_options:
          num_cpus: 4
          num_gpus: 1

        user_config:
          engine_args:
            model: NousResearch/Meta-Llama-3-8B
            served_model_name: ["Meta-Llama-3-8B"]
            download_dir: models/
            trust_remote_code: true
            tensor_parallel_size: 1
            worker_use_ray: false
            max_model_len: 8192
